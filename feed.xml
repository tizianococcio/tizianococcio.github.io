<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://tizianococcio.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tizianococcio.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-04-17T18:59:09+00:00</updated><id>https://tizianococcio.github.io/feed.xml</id><title type="html">blank</title><subtitle>Description goes here.
</subtitle><entry><title type="html">The perceptron algorithm</title><link href="https://tizianococcio.github.io/blog/2023/perceptron/" rel="alternate" type="text/html" title="The perceptron algorithm" /><published>2023-04-15T00:00:00+00:00</published><updated>2023-04-15T00:00:00+00:00</updated><id>https://tizianococcio.github.io/blog/2023/perceptron</id><content type="html" xml:base="https://tizianococcio.github.io/blog/2023/perceptron/"><![CDATA[<h1 id="introduction">Introduction</h1>

<p>The perceptron algorithm is a simple algorithm for supervised learning of binary classifiers. A binary classifier is a function that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not. The perceptron algorithm is an algorithm for learning a binary classifier from a set of training data. The perceptron algorithm is a special case of the more general linear classifier algorithm.</p>

<p>This classifier is a threshold function, a function that maps its input \(\textbf{x}\) (the vector) to an output \(f(\textbf{x})\) a single binary value:</p>

\[f(\textbf{x}) =
  \begin{cases}
    1 &amp; \text{if } \textbf{w} \cdot \textbf{x} + b &gt; 0, \\
    0 &amp; \text{otherwise} \\
  \end{cases}\]

<p>In practice, what the perceptron does is to compute the dot product between the input vector and a vector of weights, and then add a bias term. If the result is positive, the perceptron outputs 1, otherwise it outputs 0. The weights and the bias are the parameters of the perceptron, and they are learned by the perceptron algorithm.
This is equivalent to computing a linear combination of the input vector and the weights, and then adding the bias term. The core of the algorithm is to find the weights and the bias that make the perceptron output the correct value for each training example. These weights are found by starting with a random (or zero) vector of weights, and then adjusting the weights according to the following rule: if the perceptron outputs the wrong value, adjust the weights by moving them in the direction of the correct output. This is the perceptron learning rule, which can be formalized as follows:</p>

\[\textbf{w} \leftarrow \textbf{w} + \eta \left( y - \hat{y} \right) \textbf{x}\]

<p>where \(\eta\) is the learning rate, \(y\) is the correct output, \(\hat{y}\) is the perceptron’s output, and \(\textbf{x}\) is the input vector.</p>

<p>The activation function is a function that maps the output of the perceptron to a binary value. In the perceptron algorithm, the activation function is the Heaviside step function, which is defined as follows:</p>

\[\sigma(a) =
  \begin{cases}
    1 &amp; \text{if } a &gt; 0, \\
    -1 &amp; \text{otherwise} \\
  \end{cases}\]

<p>In practice, the implementation of this function for a binary classifier boils down to subtracting the output of the perceptron from the correct output, and then returning the result. This is equivalent to the following:</p>

\[\sigma(y, \hat{y}) = y - \hat{y}\]

<h1 id="python-implementation">Python implementation</h1>
<p>Note that this implementation explicitly computes the dot product between the input vector and the weights, and then adds the bias term. This is done for educational purposes, in practice the dot product is computed using the <code class="language-plaintext highlighter-rouge">np.dot</code> function, and the bias term is added using the <code class="language-plaintext highlighter-rouge">np.add</code> function.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/perceptron/perceptron.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/perceptron/perceptron.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/perceptron/perceptron.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/perceptron/perceptron.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Perceptron's decision boundary" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Animation of the decision boundary learned by the perceptron algorithm
</div>

<p>This plot is generated by the <code class="language-plaintext highlighter-rouge">fit_plot</code> function, a modified version of the <code class="language-plaintext highlighter-rouge">fit</code> function that plots the decision boundary as the training occurs. This function only handles 2-dimensional data, and it is not meant to be used in practice.</p>

<p>To generate some artificial data and train the perceptron, we can use the following code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dims</span><span class="p">))</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dims</span><span class="p">))</span>
    <span class="n">b</span> <span class="o">+=</span> <span class="mf">3.5</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">empty</span><span class="p">((</span><span class="n">n</span><span class="o">*</span><span class="mi">2</span><span class="p">,))</span>
    <span class="n">y</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">y</span><span class="p">[</span><span class="n">n</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">1</span>
    
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="c1"># Plot the data
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_data</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">100</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">100</span><span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">100</span><span class="p">:,</span> <span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<p>Here’s the full code for the perceptron algorithm:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">matplotlib.animation</span> <span class="k">as</span> <span class="n">animation</span>
<span class="kn">from</span> <span class="n">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">rc</span>

<span class="nf">rc</span><span class="p">(</span><span class="s">'animation'</span><span class="p">,</span> <span class="n">html</span><span class="o">=</span><span class="s">'html5'</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Perceptron</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="n">self</span><span class="p">.</span><span class="n">max_iterations</span> <span class="o">=</span> <span class="n">max_iterations</span>

    <span class="k">def</span> <span class="nf">dot</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="s">"""
        Dot product
        """</span>
        <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">+=</span> <span class="n">i</span><span class="o">*</span><span class="n">j</span>
        <span class="k">return</span> <span class="n">y</span>

    <span class="k">def</span> <span class="nf">sigma</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">):</span>
        <span class="s">"""
        Activation function, in practice the function has this output:
        if y == 1 and y_hat == 0:
            return 1
        if y == 0 and y_hat == 1:
            return -1
        return 0        
        """</span>
        <span class="k">return</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span>

    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># Initialize weights and bias to zero
</span>        <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="c1"># Train the perceptron for a maximum number of iterations
</span>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">max_iterations</span><span class="p">):</span>
            <span class="c1"># Loop through each training example
</span>            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="c1"># Compute the predicted class for the current example
</span>                <span class="n">predicted</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                
                <span class="c1"># Update the weights and bias
</span>                <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">sigma</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">predicted</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">sigma</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">predicted</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">fit_plot</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="s">""" Plots the decision boundary as the training occurs 
        Only works with 2-dimensional data        
        """</span>
        <span class="k">if</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">Exception</span><span class="p">(</span><span class="s">"Only words with 2-D data"</span><span class="p">)</span>
        
        <span class="c1"># plot data
</span>        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
        
        <span class="c1"># Initialize weights and bias
</span>        <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="c1"># collect data for animation
</span>        <span class="n">y_vals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])])</span>
        
        <span class="c1"># Training loop
</span>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">max_iterations</span><span class="p">):</span>
            
            <span class="c1"># Loop through each training example
</span>            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                
                <span class="c1"># Compute the predicted class for the current example
</span>                <span class="n">predicted</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                
                <span class="c1"># Update the weights and bias
</span>                <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">sigma</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">predicted</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">sigma</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">predicted</span><span class="p">)</span>   
                
                <span class="c1"># Define the decision boundary as a line in the form y = mx + b
</span>                <span class="n">m</span> <span class="o">=</span> <span class="o">-</span><span class="n">perceptron</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">perceptron</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="n">perceptron</span><span class="p">.</span><span class="n">bias</span> <span class="o">/</span> <span class="n">perceptron</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

                <span class="c1"># data to animate decision boundary plot
</span>                <span class="n">y_vals</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">m</span> <span class="o">*</span> <span class="n">x_vals</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

                
        <span class="c1"># plot decision boundary
</span>        <span class="n">self</span><span class="p">.</span><span class="n">line</span><span class="p">,</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s">'--'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

        <span class="n">ani</span> <span class="o">=</span> <span class="n">animation</span><span class="p">.</span><span class="nc">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">animate</span><span class="p">,</span> <span class="n">fargs</span><span class="o">=</span><span class="p">[</span><span class="n">y_vals</span><span class="p">],</span> <span class="n">frames</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">y_vals</span><span class="p">),</span> <span class="n">interval</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ani</span>

    <span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">line</span><span class="p">.</span><span class="nf">set_ydata</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>  <span class="c1"># update the data.
</span>        <span class="k">return</span> <span class="n">line</span><span class="p">,</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># Compute the dot product between the weights and input features
</span>        <span class="n">dot_product</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span>
        
        <span class="c1"># Threshold function: return 1 if the dot product is positive, otherwise return 0
</span>        <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">dot_product</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
</code></pre></div></div>

<h1 id="learning-logical-functions">Learning logical functions</h1>

<p>The perceptron algorithm can be used to learn logical functions, such as the AND, OR <del>and XOR</del> functions. The XOR function is not linearly separable and cannot be learned by a simple perceptron algorithm. The following code trains a perceptron to learn the AND function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">pcp</span> <span class="o">=</span> <span class="nc">Perceptron</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">pcp</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="s">"Weights: "</span><span class="p">,</span> <span class="n">pcp</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="s">"Bias: "</span><span class="p">,</span> <span class="n">pcp</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">"Predicted: "</span><span class="p">,</span> <span class="n">pcp</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="s">"Actual: "</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre></div></div>

<p>Gives the following output:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Predicted:  0 Actual:  0
Predicted:  0 Actual:  0
Predicted:  0 Actual:  0
Predicted:  1 Actual:  1
</code></pre></div></div>
<p>The perceptron correctly learns the AND function.</p>

<p>To learn the OR function, we can use the same code, but with a different set of training data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">pcp</span> <span class="o">=</span> <span class="nc">Perceptron</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">pcp</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">"Predicted: "</span><span class="p">,</span> <span class="n">pcp</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="s">"Actual: "</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre></div></div>

<h2 id="trying-to-learn-the-xor-function">Trying to learn the XOR function</h2>

<p>The XOR function is not linearly separable and cannot be learned by a simple perceptron algorithm. The following code trains a perceptron to learn the XOR function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="n">pcp</span> <span class="o">=</span> <span class="nc">Perceptron</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">pcp</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">"Predicted: "</span><span class="p">,</span> <span class="n">pcp</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="s">"Actual: "</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre></div></div>
<p>Gives the following output:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Predicted:  1 Actual:  0
Predicted:  1 Actual:  1
Predicted:  0 Actual:  1
Predicted:  1 Actual:  0
</code></pre></div></div>
<p>The perceptron does not learn the XOR function, even after 1000 iterations.</p>]]></content><author><name></name></author><category term="machine-learning" /><category term="machine-learning" /><category term="artificial-intelligence" /><category term="neural-networks" /><category term="perceptron" /><summary type="html"><![CDATA[The perceptron algorithm, an overview]]></summary></entry></feed>